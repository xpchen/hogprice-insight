"""æµ‹è¯•æ¶Œç›Šå‘¨åº¦æ•°æ®å¯¼å…¥åŠŸèƒ½ - æ”¯æŒå•ç‹¬æµ‹è¯•æŸä¸ªsheetå¹¶æ˜¾ç¤ºè¯¦ç»†è°ƒè¯•ä¿¡æ¯"""
import sys
import os
import io
import json
import hashlib
from pathlib import Path
from datetime import datetime
from sqlalchemy import text, inspect
from openpyxl import load_workbook
import pandas as pd

# è®¾ç½®UTF-8ç¼–ç è¾“å‡ºï¼ˆWindowså…¼å®¹ï¼‰
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# æ·»åŠ backendç›®å½•åˆ°è·¯å¾„ï¼ˆç¡®ä¿èƒ½å¯¼å…¥appæ¨¡å—ï¼‰
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from app.core.database import SessionLocal
from app.models.sys_user import SysUser
from app.models.dim_source import DimSource
from app.services.ingestors.unified_ingestor import unified_import
from app.services.ingestors.dispatcher import Dispatcher
from app.services.ingestors.parsers import get_parser
from app.services.ingestors.profile_loader import load_profile_from_json, get_profile_by_dataset_type
from app.services.ingestors.validator import ObservationValidator
from app.services.ingestors.error_collector import ErrorCollector
from app.models.import_batch import ImportBatch
from app.models.raw_sheet import RawSheet
from app.services.ingestors.raw_writer import save_raw_file
from app.services.ingestors.observation_upserter import upsert_observations
from app.services.sheet_table_mapper import get_table_name_for_sheet
from app.services.column_mapper import ColumnMapper
from app.services.sheet_table_importer import SheetTableImporter
from sqlalchemy import inspect, text

# é¡¹ç›®æ ¹ç›®å½•
backend_dir = Path(__file__).parent.parent
project_root = backend_dir.parent


def get_or_create_test_user(db):
    """è·å–æˆ–åˆ›å»ºæµ‹è¯•ç”¨æˆ·"""
    user = db.query(SysUser).filter(SysUser.username == "admin").first()
    if not user:
        from app.core.security import get_password_hash
        user = SysUser(
            username="test_admin",
            password_hash=get_password_hash("test123456"),
            display_name="æµ‹è¯•ç®¡ç†å‘˜",
            is_active=True
        )
        db.add(user)
        db.commit()
        db.refresh(user)
        print(f"  âœ“ åˆ›å»ºæµ‹è¯•ç”¨æˆ·: {user.username} (ID: {user.id})", flush=True)
    else:
        print(f"  âœ“ ä½¿ç”¨ç°æœ‰ç”¨æˆ·: {user.username} (ID: {user.id})", flush=True)
    return user


def print_excel_structure(file_path: Path, sheet_name: str, max_rows: int = 10):
    """æ‰“å°Excel sheetçš„å‰å‡ è¡Œç»“æ„"""
    print(f"\n{'='*80}", flush=True)
    print(f"ğŸ“Š Excel Sheetç»“æ„åˆ†æ: {sheet_name}", flush=True)
    print(f"{'='*80}", flush=True)
    
    try:
        wb = load_workbook(file_path, data_only=True)
        if sheet_name not in wb.sheetnames:
            print(f"  âŒ Sheet '{sheet_name}' ä¸å­˜åœ¨", flush=True)
            print(f"  å¯ç”¨sheets: {wb.sheetnames[:10]}", flush=True)
            return
        
        ws = wb[sheet_name]
        
        # æ‰“å°å‰max_rowsè¡Œ
        print(f"\nå‰{max_rows}è¡Œæ•°æ®:", flush=True)
        for row_idx, row in enumerate(ws.iter_rows(max_row=max_rows, values_only=False), 1):
            row_values = [cell.value if cell.value is not None else "" for cell in row[:20]]  # åªæ˜¾ç¤ºå‰20åˆ—
            row_str = " | ".join([str(v)[:15] for v in row_values])
            print(f"  è¡Œ{row_idx:2d}: {row_str}", flush=True)
        
        # æ‰“å°åˆ—åï¼ˆå°è¯•è¯†åˆ«headerè¡Œï¼‰
        print(f"\nåˆ—ååˆ†æï¼ˆå°è¯•è¯†åˆ«headerè¡Œï¼‰:", flush=True)
        for header_row_idx in [1, 2, 3, 4]:
            if header_row_idx <= ws.max_row:
                row_values = [cell.value if cell.value is not None else "" for cell in ws[header_row_idx][:20]]
                row_str = " | ".join([str(v)[:15] for v in row_values])
                print(f"  ç¬¬{header_row_idx}è¡Œ: {row_str}", flush=True)
        
        wb.close()
    except Exception as e:
        print(f"  âŒ è¯»å–Excelå¤±è´¥: {e}", flush=True)
        import traceback
        traceback.print_exc()


def test_single_sheet(file_path: Path, sheet_name: str, db, profile, uploader_id: int, auto_import: bool = False):
    """æµ‹è¯•å•ä¸ªsheetçš„è§£æå’Œå…¥åº“"""
    print(f"\n{'='*80}", flush=True)
    print(f"ğŸ§ª æµ‹è¯•Sheet: {sheet_name}", flush=True)
    print(f"{'='*80}", flush=True)
    
    # 1. æ˜¾ç¤ºExcelç»“æ„
    print_excel_structure(file_path, sheet_name, max_rows=15)
    
    # 2. åˆ›å»ºDispatcherå¹¶åˆ†æ´¾
    print(f"\n{'='*80}", flush=True)
    print(f"ğŸ” Dispatcheråˆ†æ´¾åˆ†æ", flush=True)
    print(f"{'='*80}", flush=True)
    
    dispatcher = Dispatcher(db, profile)
    
    try:
        wb = load_workbook(file_path, data_only=True)
        ws = wb[sheet_name]
        
        # è°ƒç”¨dispatcherï¼ˆä¼ å…¥Worksheetå¯¹è±¡ï¼Œä¸æ˜¯DataFrameï¼‰
        dispatch_result = dispatcher.dispatch_sheet(sheet_name, worksheet=ws)
        
        print(f"  åˆ†æ´¾ç»“æœ:", flush=True)
        print(f"    - Parser: {dispatch_result.get('parser')}", flush=True)
        sheet_config = dispatch_result.get('sheet_config', {})
        print(f"    - Sheeté…ç½®: {json.dumps(sheet_config, ensure_ascii=False, indent=2)}", flush=True)
        print(f"    - åŸå› : {dispatch_result.get('reason', 'N/A')}", flush=True)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰table_config
        table_config = sheet_config.get("table_config")
        if not table_config:
            print(f"\n  âš ï¸  è­¦å‘Š: Sheeté…ç½®ä¸­æ²¡æœ‰table_configï¼Œå°†ä½¿ç”¨æ—§æ¶æ„ï¼ˆfact_observationï¼‰", flush=True)
            print(f"  ğŸ’¡ æ£€æµ‹åˆ°é…ç½®æ–‡ä»¶å¯èƒ½å·²æ›´æ–°ï¼Œæ˜¯å¦è‡ªåŠ¨é‡æ–°åŠ è½½é…ç½®ï¼Ÿ(y/n): ", end="", flush=True)
            reload = input().strip().lower()
            if reload == 'y':
                print(f"  æ­£åœ¨é‡æ–°åŠ è½½é…ç½®...", flush=True)
                try:
                    profile = load_profile_from_json(db, profile_path)
                    db.refresh(profile)
                    # é‡æ–°dispatch
                    dispatcher = Dispatcher(db, profile)
                    dispatch_result = dispatcher.dispatch_sheet(sheet_name, worksheet=ws)
                    sheet_config = dispatch_result.get('sheet_config', {})
                    table_config = sheet_config.get("table_config")
                    if table_config:
                        print(f"  âœ“ é‡æ–°åŠ è½½æˆåŠŸï¼Œç°åœ¨ä½¿ç”¨æ–°æ¶æ„ï¼ˆç‹¬ç«‹è¡¨ï¼‰", flush=True)
                        print(f"    ç›®æ ‡è¡¨: {table_config.get('table_name', 'æœªçŸ¥')}", flush=True)
                    else:
                        print(f"  âš ï¸  é‡æ–°åŠ è½½åä»æ²¡æœ‰table_configï¼Œå°†ä½¿ç”¨æ—§æ¶æ„", flush=True)
                except Exception as e:
                    print(f"  âŒ é‡æ–°åŠ è½½å¤±è´¥: {e}", flush=True)
                    print(f"  è¯·æ‰‹åŠ¨è¿è¡Œ: python scripts/load_ingest_profiles.py", flush=True)
            else:
                print(f"  è·³è¿‡é‡æ–°åŠ è½½ï¼Œå°†ä½¿ç”¨æ—§æ¶æ„ï¼ˆfact_observationï¼‰", flush=True)
                print(f"  å¦‚éœ€ä½¿ç”¨æ–°æ¶æ„ï¼Œè¯·è¿è¡Œ: python scripts/load_ingest_profiles.py", flush=True)
        
        if not dispatch_result.get('parser'):
            print(f"\n  âš ï¸  æœªåŒ¹é…åˆ°parserï¼ŒåŸå› :", flush=True)
            print(f"    {dispatch_result.get('reason', 'æœªçŸ¥åŸå› ')}", flush=True)
            wb.close()
            return
        
        # 3. ä½¿ç”¨parserè§£æ
        parser_name = dispatch_result['parser']
        # sheet_config å·²ç»åœ¨ä¸Šé¢è·å–äº†
        
        print(f"\n{'='*80}", flush=True)
        print(f"ğŸ”§ Parserè§£æ: {parser_name}", flush=True)
        print(f"{'='*80}", flush=True)
        
        print(f"  Sheeté…ç½®:", flush=True)
        print(f"    {json.dumps(sheet_config, ensure_ascii=False, indent=4)}", flush=True)
        
        parser = get_parser(parser_name)
        if not parser:
            print(f"  âŒ Parser '{parser_name}' ä¸å­˜åœ¨", flush=True)
            wb.close()
            return
        
        # å‡†å¤‡è§£æå‚æ•°
        profile_defaults = profile.defaults_json or {}
        source_code = profile.source_code
        
        print(f"\n  å¼€å§‹è§£æ...", flush=True)
        observations = parser.parse(
            sheet_data=ws,
            sheet_config=sheet_config,
            profile_defaults=profile_defaults,
            source_code=source_code,
            batch_id=0  # ä¸´æ—¶ä½¿ç”¨0ï¼Œå…¥åº“æ—¶ä¼šåˆ›å»ºçœŸå®batch
        )
        
        print(f"\n  è§£æç»“æœ:", flush=True)
        print(f"    - è§‚æµ‹å€¼æ•°é‡: {len(observations)}", flush=True)
        
        if len(observations) == 0:
            print(f"\n  âš ï¸  è§£æå‡º0æ¡æ•°æ®ï¼Œæ— æ³•ç»§ç»­", flush=True)
            print(f"    å¯èƒ½çš„åŸå› :", flush=True)
            print(f"    1. header_rowé…ç½®ä¸æ­£ç¡®", flush=True)
            print(f"    2. æ—¥æœŸåˆ—åä¸åŒ¹é…ï¼ˆéœ€è¦'å¼€å§‹æ—¥æœŸ'å’Œ'ç»“æŸæ—¥æœŸ'ï¼‰", flush=True)
            print(f"    3. æ•°æ®è¡Œæ ¼å¼ä¸ç¬¦åˆé¢„æœŸ", flush=True)
            print(f"    4. çœä»½åˆ—è¯†åˆ«å¤±è´¥", flush=True)
            wb.close()
            return
        
        # æ˜¾ç¤ºå‰3æ¡ç¤ºä¾‹
        print(f"\n  å‰3æ¡è§‚æµ‹å€¼ç¤ºä¾‹:", flush=True)
        for i, obs in enumerate(observations[:3], 1):
            print(f"    [{i}] metric_key={obs.get('metric_key')}, geo_code={obs.get('geo_code')}, indicator={obs.get('tags', {}).get('indicator')}", flush=True)
            print(f"        {json.dumps(obs, ensure_ascii=False, indent=6, default=str)}", flush=True)
        
        # 4. æ˜¾ç¤ºç›®æ ‡è¡¨ä¿¡æ¯
        table_config = sheet_config.get("table_config")
        table_name = None
        if table_config:
            # table_nameåœ¨sheet_configé¡¶å±‚ï¼Œä¸åœ¨table_configé‡Œé¢
            table_name = sheet_config.get("table_name") or get_table_name_for_sheet(
                sheet_name=sheet_name,
                source_code=profile.source_code,
                dataset_type=profile.dataset_type
            )
            print(f"\n  ç›®æ ‡è¡¨: {table_name}", flush=True)
            print(f"  æ¶æ„: æ–°æ¶æ„ï¼ˆç‹¬ç«‹è¡¨ï¼‰", flush=True)
        else:
            print(f"\n  ç›®æ ‡è¡¨: fact_observation", flush=True)
            print(f"  æ¶æ„: æ—§æ¶æ„ï¼ˆç»Ÿä¸€è¡¨ï¼‰", flush=True)
        
        # 5. è¯¢é—®æ˜¯å¦å…¥åº“ï¼ˆå¦‚æœauto_importä¸ºFalseï¼‰
        if not auto_import:
            print(f"\n{'='*80}", flush=True)
            print(f"ğŸ’¾ å…¥åº“ç¡®è®¤", flush=True)
            print(f"{'='*80}", flush=True)
            
            confirm = input(f"æ˜¯å¦å°† {len(observations)} æ¡è§£ææ•°æ®å¯¼å…¥åˆ°æ•°æ®åº“ï¼Ÿ(y/n): ").strip().lower()
            
            if confirm != 'y' and confirm != 'æ˜¯':
                print(f"  å·²å–æ¶ˆå…¥åº“", flush=True)
                wb.close()
                return
        else:
            print(f"\n{'='*80}", flush=True)
            print(f"ğŸ’¾ è‡ªåŠ¨å…¥åº“æ¨¡å¼", flush=True)
            print(f"{'='*80}", flush=True)
            print(f"  å°†è‡ªåŠ¨å¯¼å…¥ {len(observations)} æ¡è§£ææ•°æ®åˆ°æ•°æ®åº“", flush=True)
        
        # 6. æ‰§è¡Œå…¥åº“
        print(f"\n{'='*80}", flush=True)
        print(f"ğŸš€ å¼€å§‹å…¥åº“", flush=True)
        print(f"{'='*80}", flush=True)
        
        # 6.1 åˆ›å»ºimport_batch
        with open(file_path, 'rb') as f:
            file_content = f.read()
        
        file_hash = hashlib.sha256(file_content).hexdigest()
        batch = ImportBatch(
            filename=file_path.name,
            file_hash=file_hash,
            uploader_id=uploader_id,
            source_code=profile.source_code or profile.dataset_type,
            status="processing",
            total_rows=0,
            success_rows=0,
            failed_rows=0
        )
        db.add(batch)
        db.commit()
        db.refresh(batch)
        
        print(f"  âœ“ åˆ›å»ºæ‰¹æ¬¡: ID={batch.id}", flush=True)
        
        # 6.2 éªŒè¯æ•°æ®ï¼ˆéœ€è¦batch_idï¼‰
        print(f"\n{'='*80}", flush=True)
        print(f"âœ… æ•°æ®éªŒè¯", flush=True)
        print(f"{'='*80}", flush=True)
        
        error_collector = ErrorCollector(db, batch.id)
        validator = ObservationValidator(error_collector)
        skip_metric_check = bool(table_config)
        
        valid_observations = validator.validate_batch(
            observations,
            sheet_name,
            skip_metric_key_check=skip_metric_check
        )
        
        print(f"  éªŒè¯ç»“æœ:", flush=True)
        print(f"    - æ€»è§‚æµ‹å€¼: {len(observations)} æ¡", flush=True)
        print(f"    - æœ‰æ•ˆè§‚æµ‹å€¼: {len(valid_observations)} æ¡", flush=True)
        print(f"    - æ— æ•ˆè§‚æµ‹å€¼: {len(observations) - len(valid_observations)} æ¡", flush=True)
        
        if len(valid_observations) == 0:
            print(f"\n  âš ï¸  æ²¡æœ‰æœ‰æ•ˆè§‚æµ‹å€¼ï¼Œæ— æ³•å…¥åº“", flush=True)
            if error_collector.errors:
                print(f"  å‰5ä¸ªé”™è¯¯:", flush=True)
                for error in error_collector.errors[:5]:
                    print(f"    - {error}", flush=True)
            batch.status = "failed"
            db.commit()
            wb.close()
            return
        
        # 6.2 ä¿å­˜raw_fileå’Œraw_sheet
        raw_file = save_raw_file(
            db=db,
            batch_id=batch.id,
            filename=file_path.name,
            file_content=file_content
        )
        print(f"  âœ“ ä¿å­˜raw_file: ID={raw_file.id}", flush=True)
        
        # 6.3 ä¿å­˜raw_sheetï¼ˆç®€åŒ–ç‰ˆï¼Œåªä¿å­˜å½“å‰sheetï¼‰
        raw_sheet = RawSheet(
            raw_file_id=raw_file.id,
            sheet_name=sheet_name,
            parse_status="parsed",
            parser_type=parser_name,
            observation_count=len(valid_observations)
        )
        db.add(raw_sheet)
        db.commit()
        db.refresh(raw_sheet)
        print(f"  âœ“ ä¿å­˜raw_sheet: ID={raw_sheet.id}", flush=True)
        
        # 6.4 æ›´æ–°observationsçš„batch_id
        for obs in valid_observations:
            obs['batch_id'] = batch.id
        
        # 6.5 æ ¹æ®æ¶æ„é€‰æ‹©å…¥åº“æ–¹å¼
        if table_config and valid_observations:
            # æ–°æ¶æ„ï¼šå¯¼å…¥åˆ°ç‹¬ç«‹è¡¨
            print(f"  â†’ ä½¿ç”¨æ–°æ¶æ„å¯¼å…¥åˆ°ç‹¬ç«‹è¡¨...", flush=True)
            
            column_mapping = table_config.get("column_mapping", {})
            unique_key = table_config.get("unique_key", [])
            
            # è½¬æ¢æ•°æ®
            print(f"  â†’ å¼€å§‹è½¬æ¢æ•°æ®ï¼ˆ{len(valid_observations)} æ¡è§‚æµ‹å€¼ -> è¡¨è®°å½•ï¼‰...", flush=True)
            mapper = ColumnMapper()
            records = mapper.map_observations_to_table_records(
                observations=valid_observations,
                column_mapping=column_mapping,
                table_name=table_name,
                batch_id=batch.id,
                sheet_config=sheet_config
            )
            
            print(f"  âœ“ è½¬æ¢åè®°å½•æ•°: {len(records)} æ¡", flush=True)
            
            # æ˜¾ç¤ºç¬¬ä¸€æ¡è®°å½•ç¤ºä¾‹ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            if len(records) > 0:
                print(f"\n  ç¬¬ä¸€æ¡è®°å½•ç¤ºä¾‹:", flush=True)
                print(f"    {json.dumps(records[0], ensure_ascii=False, indent=4, default=str)}", flush=True)
            
            # å¯¼å…¥åˆ°è¡¨
            print(f"  â†’ æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨...", flush=True)
            inspector = inspect(db.bind)
            if table_name not in inspector.get_table_names():
                print(f"  âŒ è¡¨ {table_name} ä¸å­˜åœ¨ï¼", flush=True)
                print(f"  ğŸ’¡ è¯·å…ˆè¿è¡Œè¿ç§»è„šæœ¬åˆ›å»ºè¡¨:", flush=True)
                print(f"     cd backend", flush=True)
                print(f"     alembic upgrade head", flush=True)
                wb.close()
                return
            
            print(f"  âœ“ è¡¨ {table_name} å­˜åœ¨", flush=True)
            print(f"  â†’ å¼€å§‹å¯¼å…¥ {len(records)} æ¡è®°å½•...", flush=True)
            
            importer = SheetTableImporter(db)
            import_result = importer.import_to_table(
                table_name=table_name,
                records=records,
                unique_key=unique_key
            )
            
            inserted = import_result.get("inserted", 0)
            updated = import_result.get("updated", 0)
            errors = import_result.get("errors", 0)
            
            print(f"  âœ“ å¯¼å…¥å®Œæˆ:", flush=True)
            print(f"    - æ’å…¥: {inserted} æ¡", flush=True)
            print(f"    - æ›´æ–°: {updated} æ¡", flush=True)
            print(f"    - é”™è¯¯: {errors} æ¡", flush=True)
            
            # ========== éªŒè¯ï¼šåŒæ—¶å¯¼å…¥åˆ°fact_observation ==========
            print(f"\n  â†’ éªŒè¯ï¼šåŒæ—¶å¯¼å…¥åˆ°fact_observation...", flush=True)
            try:
                obs_result = upsert_observations(db, valid_observations, batch_id=batch.id, sheet_name=sheet_name)
                obs_inserted = obs_result.get("inserted", 0)
                obs_updated = obs_result.get("updated", 0)
                obs_errors = obs_result.get("errors", 0)
                print(f"  âœ“ fact_observationå¯¼å…¥å®Œæˆ:", flush=True)
                print(f"    - æ’å…¥: {obs_inserted} æ¡", flush=True)
                print(f"    - æ›´æ–°: {obs_updated} æ¡", flush=True)
                print(f"    - é”™è¯¯: {obs_errors} æ¡", flush=True)
                
                # éªŒè¯metric_keyæ˜¯å¦æ­£ç¡®è®¾ç½®
                if obs_inserted > 0 or obs_updated > 0:
                    from app.models.dim_metric import DimMetric
                    from sqlalchemy import func
                    # æ£€æŸ¥æ˜¯å¦æœ‰metricè®¾ç½®äº†metric_key
                    metrics_with_key = db.query(DimMetric).filter(
                        func.json_unquote(
                            func.json_extract(DimMetric.parse_json, '$.metric_key')
                        ).isnot(None)
                    ).count()
                    print(f"  âœ“ å·²è®¾ç½®metric_keyçš„metricæ•°é‡: {metrics_with_key}", flush=True)
                    
                    # æ£€æŸ¥å½“å‰sheetç›¸å…³çš„metric
                    sheet_metrics = db.query(DimMetric).filter(
                        DimMetric.sheet_name == sheet_name
                    ).all()
                    if sheet_metrics:
                        print(f"  âœ“ å½“å‰sheetçš„metricæ•°é‡: {len(sheet_metrics)}", flush=True)
                        for metric in sheet_metrics[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                            metric_key = None
                            if metric.parse_json:
                                metric_key = metric.parse_json.get("metric_key")
                            print(f"    - {metric.metric_name}: metric_key={metric_key}", flush=True)
            except Exception as e:
                print(f"  âš ï¸  å¯¼å…¥åˆ°fact_observationå¤±è´¥: {e}", flush=True)
                import traceback
                traceback.print_exc()
            
            # éªŒè¯å¯¼å…¥ç»“æœ
            if inserted == 0 and updated == 0 and len(records) > 0:
                print(f"\n  âš ï¸  è­¦å‘Š: æ²¡æœ‰æ•°æ®è¢«æ’å…¥æˆ–æ›´æ–°", flush=True)
                print(f"  å¯èƒ½çš„åŸå› :", flush=True)
                print(f"    1. è¡¨ç»“æ„ä¸åŒ¹é…", flush=True)
                print(f"    2. å”¯ä¸€é”®å†²çªä½†æ›´æ–°å¤±è´¥", flush=True)
                print(f"    3. æ•°æ®è½¬æ¢æœ‰é—®é¢˜", flush=True)
                if len(records) > 0:
                    print(f"\n  ç¬¬ä¸€æ¡è®°å½•ç¤ºä¾‹:", flush=True)
                    print(f"    {json.dumps(records[0], ensure_ascii=False, indent=4, default=str)}", flush=True)
        else:
            # æ—§æ¶æ„ï¼šå¯¼å…¥åˆ°fact_observation
            print(f"  â†’ ä½¿ç”¨æ—§æ¶æ„å¯¼å…¥åˆ°fact_observation...", flush=True)
            
            result = upsert_observations(db, valid_observations, batch_id=batch.id, sheet_name=sheet_name)
            
            inserted = result.get("inserted", 0)
            updated = result.get("updated", 0)
            errors = result.get("errors", 0)
            
            print(f"  âœ“ å¯¼å…¥å®Œæˆ:", flush=True)
            print(f"    - æ’å…¥: {inserted} æ¡", flush=True)
            print(f"    - æ›´æ–°: {updated} æ¡", flush=True)
            print(f"    - é”™è¯¯: {errors} æ¡", flush=True)
        
        # 6.6 æ›´æ–°batchçŠ¶æ€
        batch.status = "completed"
        db.commit()
        
        print(f"\n  âœ… å…¥åº“å®Œæˆï¼æ‰¹æ¬¡ID: {batch.id}", flush=True)
        
        wb.close()
        
    except Exception as e:
        print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}", flush=True)
        import traceback
        traceback.print_exc()
        db.rollback()


def test_full_import(file_path: Path, db, uploader_id: int):
    """æµ‹è¯•å®Œæ•´å¯¼å…¥"""
    print(f"\n{'='*80}", flush=True)
    print(f"ğŸš€ å®Œæ•´å¯¼å…¥æµ‹è¯•", flush=True)
    print(f"{'='*80}", flush=True)
    
    with open(file_path, 'rb') as f:
        file_content = f.read()
    
    print(f"  æ–‡ä»¶: {file_path.name}", flush=True)
    print(f"  å¤§å°: {len(file_content) / 1024 / 1024:.2f} MB", flush=True)
    
    result = unified_import(
        db=db,
        file_content=file_content,
        filename=file_path.name,
        uploader_id=uploader_id,
        dataset_type="YONGYI_WEEKLY",
        source_code="YONGYI"
    )
    
    print(f"\n  å¯¼å…¥ç»“æœ:", flush=True)
    print(f"    - æˆåŠŸ: {result.get('success')}", flush=True)
    print(f"    - æ‰¹æ¬¡ID: {result.get('batch_id')}", flush=True)
    print(f"    - æ’å…¥æ•°: {result.get('inserted', 0)}", flush=True)
    print(f"    - æ›´æ–°æ•°: {result.get('updated', 0)}", flush=True)
    print(f"    - é”™è¯¯æ•°: {len(result.get('errors', []))}", flush=True)
    
    if result.get('errors'):
        print(f"\n  å‰5ä¸ªé”™è¯¯:", flush=True)
        for error in result.get('errors', [])[:5]:
            print(f"    - {error}", flush=True)


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    print(f"\n{'='*80}", flush=True)
    print(f"æ¶Œç›Šå‘¨åº¦æ•°æ®å¯¼å…¥æµ‹è¯•è„šæœ¬", flush=True)
    print(f"{'='*80}", flush=True)
    
    # æ–‡ä»¶è·¯å¾„
    file_path = project_root / "docs" / "2026.1.16-2026.1.22æ¶Œç›Šå’¨è¯¢ å‘¨åº¦æ•°æ®.xlsx"
    profile_path = project_root / "docs" / "ingest_profile_yongyi_weekly_v1.json"
    
    if not file_path.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}", flush=True)
        return
    
    if not profile_path.exists():
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {profile_path}", flush=True)
        return
    
    # æ•°æ®åº“è¿æ¥
    db = SessionLocal()
    try:
        # è·å–æˆ–åˆ›å»ºç”¨æˆ·
        user = get_or_create_test_user(db)
        
        # åŠ è½½profile
        print(f"\nğŸ”§ åŠ è½½å¯¼å…¥é…ç½®...", flush=True)
        profile = get_profile_by_dataset_type(db, "YONGYI_WEEKLY")
        if not profile:
            print(f"  âš ï¸  æ•°æ®åº“ä¸­æœªæ‰¾åˆ°profileï¼Œä»JSONæ–‡ä»¶åŠ è½½...", flush=True)
            profile = load_profile_from_json(db, profile_path)
            if not profile:
                print(f"  âŒ åŠ è½½profileå¤±è´¥", flush=True)
                return
        else:
            print(f"  âœ“ Profileå·²å­˜åœ¨: {profile.profile_code}", flush=True)
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åŠ è½½é…ç½®ï¼ˆæ£€æŸ¥æ˜¯å¦æœ‰sheetç¼ºå°‘table_configï¼‰
            need_reload = False
            if profile.sheets:
                sheets_without_table_config = [
                    s.sheet_name for s in profile.sheets 
                    if s.config_json and not s.config_json.get("table_config") and s.parser not in ['RAW_TABLE_STORE_ONLY', 'SKIP_META']
                ]
                if sheets_without_table_config:
                    print(f"  âš ï¸  å‘ç° {len(sheets_without_table_config)} ä¸ªsheetç¼ºå°‘table_configé…ç½®", flush=True)
                    print(f"  ğŸ’¡ æç¤º: é…ç½®æ–‡ä»¶å¯èƒ½å·²æ›´æ–°ï¼Œå»ºè®®é‡æ–°åŠ è½½é…ç½®", flush=True)
                    print(f"     è¿è¡Œ: python scripts/load_ingest_profiles.py", flush=True)
        
        print(f"  âœ“ Profileå·²åŠ è½½: {profile.profile_code} ({len(profile.sheets) if profile.sheets else 0} sheets)", flush=True)
        
        # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
        if len(sys.argv) > 1:
            # å‘½ä»¤è¡Œæ¨¡å¼ï¼šç›´æ¥æµ‹è¯•æŒ‡å®šçš„sheet
            sheet_name_arg = sys.argv[1]
            # æ£€æŸ¥æ˜¯å¦æœ‰è‡ªåŠ¨å…¥åº“å‚æ•°ï¼ˆå¯èƒ½åœ¨åŒä¸€ä¸ªå‚æ•°ä¸­ï¼Œç”¨ç©ºæ ¼åˆ†éš”ï¼‰
            auto_import = False
            if len(sys.argv) > 2:
                auto_flag = sys.argv[2].lower()
                if auto_flag in ['-y', '--yes', '--auto', 'y', 'æ˜¯']:
                    auto_import = True
            elif ' ' in sheet_name_arg:
                # æ£€æŸ¥æ˜¯å¦åœ¨sheetåç§°å‚æ•°ä¸­åŒ…å«äº†è‡ªåŠ¨å…¥åº“æ ‡å¿—
                parts = sheet_name_arg.split(' ', 1)
                sheet_name_arg = parts[0]
                if len(parts) > 1 and parts[1].lower() in ['-y', '--yes', '--auto', 'y', 'æ˜¯']:
                    auto_import = True
            
            # å°è¯•ä»Excelæ–‡ä»¶ä¸­æŸ¥æ‰¾åŒ¹é…çš„sheetåç§°ï¼ˆå¤„ç†ç¼–ç é—®é¢˜ï¼‰
            wb_temp = load_workbook(file_path, data_only=True)
            sheet_name = None
            # ç²¾ç¡®åŒ¹é…
            if sheet_name_arg in wb_temp.sheetnames:
                sheet_name = sheet_name_arg
            else:
                # æ¨¡ç³ŠåŒ¹é…ï¼ˆå¤„ç†ç¼–ç é—®é¢˜ï¼‰
                for actual_sheet_name in wb_temp.sheetnames:
                    if sheet_name_arg in actual_sheet_name or actual_sheet_name in sheet_name_arg:
                        sheet_name = actual_sheet_name
                        break
                if not sheet_name:
                    print(f"  âŒ æœªæ‰¾åˆ°åŒ¹é…çš„sheet: '{sheet_name_arg}'", flush=True)
                    print(f"  å¯ç”¨sheetsï¼ˆå‰10ä¸ªï¼‰:", flush=True)
                    for i, name in enumerate(wb_temp.sheetnames[:10], 1):
                        print(f"    {i:2d}. {name}", flush=True)
                    wb_temp.close()
                    return
            wb_temp.close()
            
            print(f"\nğŸ“‹ å‘½ä»¤è¡Œæ¨¡å¼: æµ‹è¯•sheet '{sheet_name}'", flush=True)
            if auto_import:
                print(f"  âœ“ è‡ªåŠ¨å…¥åº“æ¨¡å¼å·²å¯ç”¨", flush=True)
            test_single_sheet(file_path, sheet_name, db, profile, user.id, auto_import=auto_import)
        else:
            # äº¤äº’å¼é€‰æ‹©æµ‹è¯•æ¨¡å¼
            print(f"\n{'='*80}", flush=True)
            print(f"è¯·é€‰æ‹©æµ‹è¯•æ¨¡å¼:", flush=True)
            print(f"  1. æµ‹è¯•å•ä¸ªsheetï¼ˆäº¤äº’å¼ï¼‰", flush=True)
            print(f"  2. æµ‹è¯•å®Œæ•´å¯¼å…¥", flush=True)
            print(f"  3. åˆ—å‡ºæ‰€æœ‰sheets", flush=True)
            print(f"{'='*80}", flush=True)
            
            choice = input("è¯·è¾“å…¥é€‰é¡¹ (1/2/3): ").strip()
            
            if choice == "1":
                # åˆ—å‡ºæ‰€æœ‰sheets
                wb = load_workbook(file_path, data_only=True)
                print(f"\nå¯ç”¨sheets:", flush=True)
                for i, sheet_name in enumerate(wb.sheetnames, 1):
                    print(f"  {i:2d}. {sheet_name}", flush=True)
                wb.close()
                
                sheet_input = input("\nè¯·è¾“å…¥sheetåç§°æˆ–åºå·: ").strip()
                
                # å°è¯•æŒ‰åºå·æŸ¥æ‰¾
                try:
                    sheet_idx = int(sheet_input) - 1
                    wb = load_workbook(file_path, data_only=True)
                    if 0 <= sheet_idx < len(wb.sheetnames):
                        sheet_name = wb.sheetnames[sheet_idx]
                    else:
                        sheet_name = sheet_input
                    wb.close()
                except ValueError:
                    sheet_name = sheet_input
                
                test_single_sheet(file_path, sheet_name, db, profile, user.id, auto_import=False)
                
            elif choice == "2":
                test_full_import(file_path, db, user.id)
                
            elif choice == "3":
                wb = load_workbook(file_path, data_only=True)
                print(f"\næ‰€æœ‰sheets ({len(wb.sheetnames)}ä¸ª):", flush=True)
                for i, sheet_name in enumerate(wb.sheetnames, 1):
                    print(f"  {i:2d}. {sheet_name}", flush=True)
                wb.close()
            else:
                print(f"âŒ æ— æ•ˆé€‰é¡¹", flush=True)
    
    finally:
        db.close()


if __name__ == "__main__":
    main()
